import logging
import json
import pickle
import os
import time
from typing import Dict, Any, List

from src.core.storage.walrus_client import WalrusClient
from src.core.encryption.local_crypto import LocalCrypto
from src.core.ml.feature_extractor import extract, vectorize
from sklearn.ensemble import IsolationForest

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Trainer")

# Model dosyasÄ±nÄ±n yolu
MODEL_PATH = "src/core/models/model.pkl"

class Trainer:
    """
    Automated Feedback Loop:
    Walrus -> Download -> Decrypt -> Preprocess -> Retrain -> Save
    """
    def __init__(self):
        self.storage = WalrusClient()
        self.crypto = LocalCrypto()
        
    def train_from_walrus(self, blob_id: str) -> Dict[str, Any]:
        """
        Åifreli Walrus blob'unu indirir, Ã§Ã¶zer ve modeli eÄŸitir.
        """
        logger.info(f"ğŸ“ Training started for Blob ID: {blob_id}")
        
        # 1. DOWNLOAD
        encrypted_bytes = self.storage.read_blob(blob_id)
        if not encrypted_bytes:
            logger.error("âŒ Download failed. Aborting training.")
            return {"status": "failed", "reason": "download_error"}
            
        # 2. DECRYPT
        try:
            # Bytes -> JSON String (Encrypted Payload)
            payload_str = encrypted_bytes.decode('utf-8')
            encrypted_payload = json.loads(payload_str)
            
            # Decrypt -> JSON String (Bundle)
            plaintext = self.crypto.decrypt(encrypted_payload)
            bundle = json.loads(plaintext)
            
            logger.info(f"ğŸ”“ Decryption successful. Found {bundle.get('record_count')} records.")
            
        except Exception as e:
            logger.error(f"âŒ Decryption failed: {e}")
            return {"status": "failed", "reason": "decryption_error"}
            
        # 3. PREPROCESS (Feature Extraction)
        logs = bundle.get("logs", [])
        if not logs:
            logger.warning("Empty batch. Skipping training.")
            return {"status": "skipped", "reason": "empty_batch"}
            
        X_train = []
        for log in logs:
            # Training iÃ§in "state" bilgisi simÃ¼le edilir veya boÅŸ geÃ§ilir.
            # Batch process olduÄŸu iÃ§in gerÃ§ek zamanlÄ± state tam olarak bilinemez.
            # Ancak stateless featurelar (meter_value, msg_type) yeterlidir.
            features = extract(log, state={}) 
            vector = vectorize(features)
            X_train.append(vector)
            
        if not X_train:
            return {"status": "skipped", "reason": "no_features"}
            
        # 4. TRAIN (Retrain from scratch or partial_fit if supported)
        # IsolationForest partial_fit desteklemez (sklearn standart).
        # Bu yÃ¼zden elimizdeki son batch ile "Incremental Learning" simÃ¼lasyonu yapÄ±yoruz
        # veya sÄ±fÄ±rdan eÄŸitiyoruz. GerÃ§ek dÃ¼nyada bÃ¼yÃ¼k veri seti birikmeli.
        # Burada demo amaÃ§lÄ±: Sadece bu batch ile modeli gÃ¼ncelliyoruz (Overfit riski var ama akÄ±ÅŸ doÄŸru)
        
        logger.info(f"ğŸ§  Training model with {len(X_train)} samples...")
        
        # Daha saÄŸlam model iÃ§in parametreler
        model = IsolationForest(
            n_estimators=100,
            contamination=0.05, # %5 anomali varsayÄ±mÄ±
            random_state=42
        )
        model.fit(X_train)
        
        # 5. SAVE
        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        
        model_data = {
            "model": model,
            "feature_order": ["msg_type_hash", "has_meter", "..."], # Tam listeyi feature_extractor'dan almalÄ±
            "contamination": 0.05,
            "train_samples": len(X_train),
            "timestamp": time.time(),
            "source_blob": blob_id
        }
        
        try:
            with open(MODEL_PATH, "wb") as f:
                pickle.dump(model_data, f)
            logger.info(f"ğŸ’¾ Model saved to {MODEL_PATH}")
        except Exception as e:
            logger.error(f"âŒ Failed to save model: {e}")
            return {"status": "failed", "reason": "save_error"}
            
        return {
            "status": "success",
            "samples": len(X_train),
            "blob_id": blob_id,
            "model_path": MODEL_PATH
        }

if __name__ == "__main__":
    # Test
    # t = Trainer()
    # t.train_from_walrus("BLOB_ID_HERE")
    pass
